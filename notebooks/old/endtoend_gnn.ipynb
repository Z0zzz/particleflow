{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import EdgeConv, MessagePassing, EdgePooling\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit\n",
    "def regularize_X_y(X_elements, y_candidates, X_element_block_id, y_candidate_block_id):\n",
    "    ret_x = np.zeros_like(X_elements)\n",
    "    ret_x_id = np.zeros_like(X_element_block_id)\n",
    "    ret_y = np.zeros((X_elements.shape[0], y_candidates.shape[1]))\n",
    "    ret_y_id = np.zeros((X_elements.shape[0]), dtype=np.int64)\n",
    "    \n",
    "    idx = 0\n",
    "    for cl in np.unique(X_element_block_id):\n",
    "        m1 = X_element_block_id == cl\n",
    "        m2 = y_candidate_block_id == cl\n",
    "\n",
    "        x = X_elements[m1]\n",
    "        y = y_candidates[m2]\n",
    "\n",
    "        n = x.shape[0]\n",
    "        ret_x[idx:idx+n] = x[:]\n",
    "        ret_x_id[idx:idx+n] = cl\n",
    "        ret_y[idx:idx+y.shape[0]] = y[:]\n",
    "        ret_y_id[idx:idx+n] = cl\n",
    "        \n",
    "        idx += n\n",
    "        \n",
    "    return ret_x, ret_y, ret_x_id, ret_y_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFGraphDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, connect_all=False, max_elements=None, max_candidates=None):\n",
    "        self._connect_all = connect_all\n",
    "        self._max_elements = max_elements\n",
    "        self._max_candidates = max_candidates\n",
    "        super(PFGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.raw_dir = root\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_list = glob(self.raw_dir+'/*ev*.npz')\n",
    "        return sorted([l.replace(self.raw_dir,'.') for l in raw_list])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_{}.pt'.format(i) for i in range(len(self.raw_file_names))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        feature_scale = np.array([1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "        i = 0\n",
    "        \n",
    "        for raw_file_name in self.raw_file_names:\n",
    "            if i%100 == 0:\n",
    "                print(i, len(self.raw_file_names))\n",
    "            \n",
    "            dist_file_name = raw_file_name.replace('ev','dist')\n",
    "            #print(\"loading data from files: {0}, {1}\".format(osp.join(self.raw_dir, raw_file_name), osp.join(self.raw_dir, dist_file_name)))\n",
    "            try:\n",
    "                fi = np.load(osp.join(self.raw_dir, raw_file_name))\n",
    "                fi_dist = np.load(osp.join(self.raw_dir, dist_file_name))\n",
    "            except Exception as e:\n",
    "                print(\"Could not open files: {0}, {1}\".format(osp.join(self.raw_dir, raw_file_name), osp.join(self.raw_dir, dist_file_name)))\n",
    "                continue\n",
    "            \n",
    "            X_elements = fi['elements'][:self._max_elements]\n",
    "            X_element_block_id = fi['element_block_id'][:self._max_elements]\n",
    "            y_candidates = fi['candidates'][:self._max_candidates, 1:]\n",
    "            y_candidate_block_id = fi['candidate_block_id'][:self._max_candidates]\n",
    "            \n",
    "            X_elements, y_candidates, X_element_block_id, y_candidate_block_id = regularize_X_y(\n",
    "                X_elements, y_candidates, X_element_block_id, y_candidate_block_id)\n",
    "            \n",
    "            num_elements = X_elements.shape[0]\n",
    "\n",
    "            row_index = fi_dist['row']\n",
    "            col_index = fi_dist['col']\n",
    "            num_edges = row_index.shape[0]\n",
    "\n",
    "            edge_index = np.zeros((2, 2*num_edges))\n",
    "            edge_index[0,:num_edges] = row_index\n",
    "            edge_index[1,:num_edges] = col_index\n",
    "            edge_index[0,num_edges:] = col_index\n",
    "            edge_index[1,num_edges:] = row_index\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "            edge_data = fi_dist['data']\n",
    "            edge_attr = np.zeros((2*num_edges,1))\n",
    "            edge_attr[:num_edges,0] = edge_data\n",
    "            edge_attr[num_edges:,0] = edge_data\n",
    "            edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "            x = torch.tensor(X_elements/feature_scale, dtype=torch.float)\n",
    "\n",
    "            #y = [X_element_block_id[i]==X_element_block_id[j] for (i,j) in edge_index.t().contiguous()]\n",
    "            y = torch.tensor(y_candidates, dtype=torch.float)\n",
    "            \n",
    "            data = Data(x=x, edge_index=edge_index, y=y, edge_attr=edge_attr)\n",
    "            data.x_cluster_labels = torch.tensor(X_element_block_id, dtype=torch.float)\n",
    "            data.y_cluster_labels = torch.tensor(y_candidate_block_id, dtype=torch.float)\n",
    "#             data.y_cluster_labels = torch.nn.functional.pad(\n",
    "#                 data.y_cluster_labels, (0, x.shape[0] - data.y_cluster_labels.shape[0]),\n",
    "#                 value=-1)\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(i)))\n",
    "            i += 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(idx)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ../data/TTBar_run3/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = PFGraphDataset(root='../data/TTbar_run3')\n",
    "#full_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_dataset.get(0)\n",
    "input_dim = data.x.shape[1]\n",
    "edge_dim = data.edge_attr.shape[1]\n",
    "\n",
    "batch_size = 4\n",
    "n_epochs = 50\n",
    "lr = 1e-5\n",
    "hidden_dim = 128\n",
    "n_iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConvWithEdgeAttr(MessagePassing):\n",
    "    def __init__(self, nn, aggr='max', **kwargs):\n",
    "        super(EdgeConvWithEdgeAttr, self).__init__(aggr=aggr, **kwargs)\n",
    "        self.nn = nn\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\"\"\"\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        pseudo = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, pseudo=pseudo)\n",
    "\n",
    "    def message(self, x_i, x_j, pseudo):\n",
    "        return self.nn(torch.cat([x_i, x_j - x_i, pseudo], dim=1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.nn)\n",
    "\n",
    "class PFNet1(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, edge_dim=1, output_dim=1, n_iters=1, aggr='add'):\n",
    "        super(PFNet1, self).__init__()\n",
    "        \n",
    "        convnn = nn.Sequential(nn.Linear(2*(hidden_dim + input_dim)+edge_dim, 2*hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.ReLU()\n",
    "        )\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(40)\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "#         self.edgenetwork = nn.Sequential(nn.Linear(2*(hidden_dim+input_dim)+edge_dim,2*hidden_dim),\n",
    "#                                          nn.ReLU(),\n",
    "#                                          nn.Linear(2*hidden_dim, output_dim),\n",
    "#                                          nn.Sigmoid())\n",
    "\n",
    "        self.nodenetwork = EdgeConvWithEdgeAttr(nn=convnn, aggr=aggr)\n",
    "        \n",
    "        self.pooling1 = EdgePooling(40, dropout=0.2)\n",
    "        self.pooling2 = EdgePooling(40, dropout=0.2)\n",
    "        self.pooling3 = EdgePooling(40, dropout=0.2)\n",
    "        \n",
    "        self.outnetwork = nn.Sequential(nn.Linear(40, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):        \n",
    "        X = self.batchnorm1(data.x)\n",
    "        H = self.inputnet(X)\n",
    "        x = torch.cat([H,X],dim=-1)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            H = self.nodenetwork(x, data.edge_index, data.edge_attr)\n",
    "            x = torch.cat([H,X],dim=-1)\n",
    "\n",
    "        #row,col = data.edge_index        \n",
    "        #output = self.edgenetwork(torch.cat([x[row], x[col], data.edge_attr],dim=-1)).squeeze(-1)\n",
    "\n",
    "        pooled, edge_index, batch1, unpool_info1 = self.pooling1(x, data.edge_index, data.batch)\n",
    "        pooled, edge_index, batch2, unpool_info2 = self.pooling2(pooled, edge_index, batch1)\n",
    "        pooled, edge_index, batch3, unpool_info3 = self.pooling3(pooled, edge_index, batch2)\n",
    "        \n",
    "        r = self.outnetwork(self.batchnorm2(pooled))\n",
    "        \n",
    "        return r, unpool_info1.cluster, unpool_info2.cluster, unpool_info3.cluster\n",
    "    \n",
    "class PFNet2(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, edge_dim=1, output_dim=1, n_iters=1, aggr='mean'):\n",
    "        super(PFNet2, self).__init__()\n",
    "        \n",
    "        convnn = nn.Sequential(nn.Linear(2*(hidden_dim + input_dim)+edge_dim, 2*hidden_dim),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.LeakyReLU(),\n",
    "        )\n",
    "        convnn2 = nn.Sequential(nn.Linear(2*(hidden_dim + input_dim)+edge_dim, 2*hidden_dim),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(hidden_dim, 3),\n",
    "        )\n",
    "\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_dim + input_dim)\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.nodenetwork = EdgeConvWithEdgeAttr(nn=convnn, aggr=aggr)\n",
    "        self.nodenetwork2 = EdgeConvWithEdgeAttr(nn=convnn2, aggr=aggr)\n",
    "        \n",
    "#         self.outnetwork = nn.Sequential(nn.Linear(hidden_dim + input_dim, 64),\n",
    "#                                nn.LeakyReLU(),\n",
    "#                                nn.Linear(64, 32),\n",
    "#                                nn.LeakyReLU(),\n",
    "#                                nn.Linear(32, 16),\n",
    "#                                nn.LeakyReLU(),\n",
    "#                                nn.Linear(16, 3),\n",
    "#         )\n",
    "\n",
    "    def forward(self, data):        \n",
    "        X = self.batchnorm1(data.x)\n",
    "        H = self.inputnet(X)\n",
    "        x = torch.cat([H,X],dim=-1)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            x = self.batchnorm2(x)\n",
    "            H = self.nodenetwork(x, data.edge_index, data.edge_attr)\n",
    "            x = torch.cat([H,X],dim=-1)\n",
    "                \n",
    "        #r = self.outnetwork(x)\n",
    "        r = self.nodenetwork2(x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit\n",
    "def reorder_predicted_target(predicted_y, target_y, x_cluster_labels, y_cluster_labels, p1, p2, p3):\n",
    "    maxvals = max(predicted_y.shape[0], target_y.shape[0])\n",
    "    chosen_pred = np.zeros(maxvals, dtype=np.int32)\n",
    "    chosen_target = np.zeros(maxvals, dtype=np.int32)\n",
    "    \n",
    "    idx = 0\n",
    "    for cl in np.unique(y_cluster_labels):\n",
    "        m1 = y_cluster_labels == cl\n",
    "        m2 = x_cluster_labels == cl\n",
    "\n",
    "        #get the predicted and target candidates that use elements from this block\n",
    "        pred = p3[p2[p1[m2]]]\n",
    "        tgt = np.where(m1)[0]\n",
    "        n = min(pred.shape[0], tgt.shape[0])\n",
    "\n",
    "        chosen_pred[idx:idx+n] = pred[:n]\n",
    "        chosen_target[idx:idx+n] = tgt[:n]\n",
    "        \n",
    "        idx += n\n",
    "    return chosen_pred[:idx], chosen_target[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(full_dataset, batch_size=1, pin_memory=True, shuffle=False)\n",
    "# model = PFNet1(input_dim=input_dim, hidden_dim=hidden_dim, edge_dim=edge_dim, n_iters=n_iters).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "# loss = torch.nn.MSELoss()\n",
    "\n",
    "# # print(model)\n",
    "# # model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# # params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# # print(params)\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# losses = []\n",
    "# corrs = []\n",
    "# t0 = time.time()\n",
    "\n",
    "# for j in range(200):\n",
    "#     losses_batch = []\n",
    "#     corrs_batch = []\n",
    "    \n",
    "#     num_pred = []\n",
    "#     num_true = []\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         if i>200:\n",
    "#             break\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         batch_target = data.y        \n",
    "#         batch_output, pool_clusters1, pool_clusters2, pool_clusters3 = model(data)\n",
    "        \n",
    "#         #Find the correspondence between the predicted candidates and true candidates based\n",
    "#         #on the block id of the input elements\n",
    "#         chosen_pred, chosen_target = reorder_predicted_target(\n",
    "#             batch_output.detach().cpu().numpy(),\n",
    "#             data.y.detach().cpu().numpy(),\n",
    "#             data.x_cluster_labels.detach().cpu().numpy(),\n",
    "#             data.y_cluster_labels.detach().cpu().numpy(),\n",
    "#             pool_clusters1.detach().cpu().numpy(),\n",
    "#             pool_clusters2.detach().cpu().numpy(),\n",
    "#             pool_clusters3.detach().cpu().numpy()\n",
    "#         )\n",
    "        \n",
    "#         #Create arrays where each row corresponds to a matched true or predicted candidate\n",
    "#         preds_cleaned = batch_output[torch.tensor(chosen_pred, dtype=torch.long)]\n",
    "#         targets_cleaned = data.y[torch.tensor(chosen_target, dtype=torch.long)]\n",
    "#         batch_loss = loss(\n",
    "#             preds_cleaned,\n",
    "#             targets_cleaned\n",
    "#         )\n",
    "        \n",
    "#         batch_loss.backward()\n",
    "#         batch_loss_item = batch_loss.item()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         corr_pt = np.corrcoef(\n",
    "#             preds_cleaned[:, 0].detach().cpu().numpy(),\n",
    "#             targets_cleaned[:, 0].detach().cpu().numpy())[0,1]\n",
    "#         corrs_batch += [corr_pt]\n",
    "#         losses_batch += [batch_loss_item]\n",
    "    \n",
    "#     l = np.mean(losses_batch)\n",
    "#     losses += [l]\n",
    "#     corrs += [np.mean(corrs_batch)]\n",
    "#     t1 = time.time()\n",
    "#     print(\"epoch={0}, dt={1:.1f}s, loss={2:.4f}, corr_pt={3:.4f}\".format(j, t1 - t0, losses[-1], corrs[-1]))\n",
    "#     t0 = t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)\n",
    "model = PFNet2(input_dim=input_dim, hidden_dim=hidden_dim, edge_dim=edge_dim, n_iters=n_iters).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss2 = torch.nn.BCELoss()\n",
    "\n",
    "print(model)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"params\", params)\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "corrs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for j in range(n_epochs):\n",
    "    losses_batch = []\n",
    "    corrs_batch = []\n",
    "    \n",
    "    num_pred = []\n",
    "    num_true = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        if i>500:\n",
    "            break\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        batch_loss = loss(\n",
    "            output,\n",
    "            data.y\n",
    "        )\n",
    "                  \n",
    "        if i==0 and j%10==0:\n",
    "            #print(is_pred.detach().cpu().numpy())\n",
    "            #print((data.y[:, 0]!=0).to(dtype=torch.float).detach().cpu().numpy())\n",
    "            print(output[:5].detach().cpu().numpy())\n",
    "            print(data.y[:5].detach().cpu().numpy())\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        batch_loss_item = batch_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        corr_pt = np.corrcoef(\n",
    "            output[:, 0].detach().cpu().numpy(),\n",
    "            data.y[:, 0].detach().cpu().numpy())[0,1]\n",
    "        corrs_batch += [corr_pt]\n",
    "        losses_batch += [batch_loss_item]\n",
    "    \n",
    "    l = np.mean(losses_batch)\n",
    "    losses += [l]\n",
    "    corrs += [np.mean(corrs_batch)]\n",
    "    t1 = time.time()\n",
    "    print(\"epoch={0}, dt={1:.1f}s, loss={2:.4f}, corr_pt={3:.4f}\".format(j, t1 - t0, losses[-1], corrs[-1]))\n",
    "    t0 = t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylim(0.8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.to(device=device)\n",
    "output = model(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = (data.x_cluster_labels == 0) & (data.batch == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x[cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y[cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "msk = data.y[:, 0] != 0\n",
    "plt.scatter(\n",
    "    data.y[msk][:, 0].detach().cpu().numpy(),\n",
    "    output[msk][:, 0].detach().cpu().numpy(),\n",
    "    marker=\".\", alpha=0.5)\n",
    "plt.plot([0,5],[0,5])\n",
    "plt.xlim(0,5)\n",
    "plt.ylim(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(\n",
    "    data.y[msk, 1].detach().cpu().numpy(),\n",
    "    output[msk, 1].detach().cpu().numpy(),\n",
    "    marker=\".\", alpha=0.5)\n",
    "plt.plot([-5,5],[-5,5])\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(\n",
    "    data.y[msk, 2].detach().cpu().numpy(),\n",
    "    output[msk, 2].detach().cpu().numpy(),\n",
    "    marker=\".\", alpha=0.5)\n",
    "plt.plot([-5,5],[-5,5])\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0,10,40)\n",
    "plt.hist(data.y[msk, 0].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(output[msk, 0].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-5,5,40)\n",
    "plt.hist(data.y[msk, 1].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(output[msk, 1].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-5,5,40)\n",
    "plt.hist(data.y[msk, 2].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(output[msk, 2].detach().cpu().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
