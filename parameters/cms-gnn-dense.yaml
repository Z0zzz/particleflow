backend: tensorflow

dataset:
  schema: cms
  target_particles: cand
  num_input_features: 15
  num_output_features: 7
#       NONE = 0,
#       TRACK = 1,
#       PS1 = 2,
#       PS2 = 3,
#       ECAL = 4,
#       HCAL = 5,
#       GSF = 6,
#       BREM = 7,
#       HFEM = 8,
#       HFHAD = 9,
#       SC = 10,
#       HO = 11,
  num_input_classes: 12
  #(none=0, ch.had=1, n.had=2, hfem=3, hfhad=4, gamma=5, e=6, mu=7)
  num_output_classes: 8
  padded_num_elem_size: 6400
  #(pt, eta, sin phi, cos phi, E)
  num_momentum_outputs: 5
  classification_loss_coef: 5.0
  charge_loss_coef: 0.1
  pt_loss_coef: 1.0
  eta_loss_coef: 0.1
  sin_phi_loss_coef: 1.0
  cos_phi_loss_coef: 1.0
  energy_loss_coef: 0.01
  raw_path: data/TTbar_14TeV_TuneCUETP8M1_cfi/raw/*.pkl.bz2
  processed_path: data/TTbar_14TeV_TuneCUETP8M1_cfi/tfr_cand/*.tfrecords
  num_files_per_chunk: 1
  validation_file_path: data/TTbar_14TeV_TuneCUETP8M1_cfi/val/*.pkl.bz2

tensorflow:
  eager: no

setup:
  train: yes
  weights:
  weights_config:
  lr: 1e-4
  batch_size: 5
  num_events_train: 80000
  num_events_test: 10000
  num_epochs: 500
  num_val_files: 100
  dtype: float32
  trainable: all
  classification_loss_type: categorical_cross_entropy
  lr_schedule: exponentialdecay  # exponentialdecay, onecycle
  optimizer: adam  # adam, adamw, sgd

optimizer:
  adam:
    amsgrad: no
  adamw:
    amsgrad: yes
    weight_decay: 0.001
  sgd:
    nesterov: no
    momentum: 0.9

sample_weights:
  cls: inverse_sqrt
  charge: signal_only
  pt: signal_only
  eta: signal_only
  sin_phi: signal_only
  cos_phi: signal_only
  energy: signal_only

parameters:
  model: gnn_dense
  activation: elu
  layernorm: no
  hidden_dim: 256
  bin_size: 640
  clip_value_low: 0.01
  num_conv: 2
  num_gsl: 2
  normalize_degrees: yes
  distance_dim: 128
  dropout: 0.2
  separate_momentum: yes
  input_encoding: cms
  debug: no

timing:
  num_ev: 100
  num_iter: 3

exponentialdecay:
  decay_steps: 10000
  decay_rate: 0.99
  staircase: yes

callbacks:
  checkpoint:
    save_weights_only: yes
    monitor: "val_loss"
    save_best_only: no
  draw_events: no
  tensorboard:
    dump_history: yes
    hist_freq: 1

hypertune:
  algorithm: hyperband  # random, bayesian, hyperband
  random:
    objective: val_loss
    max_trials: 100
  bayesian:
    objective: val_loss
    max_trials: 100
    num_initial_points: 2
  hyperband:
    objective: val_loss
    max_epochs: 500
    factor: 2
    iterations: 1
    executions_per_trial: 1

raytune:
  local_dir:  # Note: please specify an absolute path
  sched: "asha"  # asha, hyperband
  parameters:
    # optimizer parameters
    lr: [1e-4]
    batch_size: [32]
    expdecay_decay_steps: [10000]
    # model parameters
    layernorm: [False]
    hidden_dim: [256]
    distance_dim: [128, 256]
    num_conv: [2, 3, 4]
    num_gsl: [2, 3, 4]
    dropout: [0.0, 0.1]
    bin_size: [640]
    clip_value_low: [0.0]
    normalize_degrees: [True]
  # Tune schedule specific parameters
  asha:
    max_t: 10
    reduction_factor: 3
    brackets: 1
    grace_period: 5
  hyperband:
    max_t: 10
    reduction_factor: 3
